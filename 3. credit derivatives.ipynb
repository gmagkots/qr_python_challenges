{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6499355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcf45b2",
   "metadata": {},
   "source": [
    "___\n",
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8f6701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rv_trade_count(df):\n",
    "    \"\"\"\n",
    "    Process the data with identical timestamps to identify RV trades.\n",
    "\n",
    "    :param df: dataframe with candidate RV trades\n",
    "    :return: dataframe with indicator of daily RV trades,\n",
    "             dataframe with timestamps of trade pairs\n",
    "    \"\"\"\n",
    "    # get all rows with duplicate timestamps\n",
    "    dft = df[df.duplicated(subset=['timestamp'], keep=False)]\n",
    "\n",
    "    # group by timestamp and count the tickers in each group, ignore the other \n",
    "    # identifying variable log_sn for now. The rows with duplicate timestamps \n",
    "    # will include only the valid security pairs, because duplicates rows with \n",
    "    # only ITXEB5 trades are aggregated to a single row by value_counts.\n",
    "    mask_cols = ['timestamp', 'ticker']\n",
    "    mask = dft[mask_cols].groupby('timestamp').value_counts().reset_index()\n",
    "    mask = mask[mask.duplicated(subset=['timestamp'], keep=False)]\n",
    "\n",
    "    # get the log_sn values and calculate their differences for the candidate pairs\n",
    "    # dft = pd.merge(dft[mask_cols + ['date', 'log_sn']], mask[mask_cols], \n",
    "    #                how='inner', on=mask_cols)\n",
    "    dft = pd.merge(dft, mask[mask_cols], how='inner', on=mask_cols)\n",
    "    dft['Dlog_sn'] = dft.groupby('timestamp')['log_sn'].diff().abs()\n",
    "\n",
    "    # identify valid trade pairs with identical timestamps by enforcing the\n",
    "    # spread/notional similarity criteria Δlog(SxN) <= 0.5\n",
    "    dft = dft[dft['Dlog_sn'] <= 0.5]\n",
    "\n",
    "    # get the minimum daily number of RV trades and the detailed timestamps\n",
    "    # of confirmed trades (those with identical timestamps) during the day\n",
    "    out = dft.groupby('date')['Dlog_sn'].count().rename('RV_trades').reset_index()\n",
    "    tstamps = dft[['date', 'timestamp']]\n",
    "\n",
    "    return out, tstamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3386760b",
   "metadata": {},
   "source": [
    "___\n",
    "### Data parsing and processing\n",
    "\n",
    "**CDXIG5 notional left tail:**  \n",
    "The notional value for CDXIG5 has a more skewed left tail than the other two securities. Instead of coding a process for outlier identification and removal (e.g. by using the median absolute deviation or other criteria), it is more practical to cut off CDXIG5's nominal to values above the approximate minimum for the other two securities. The reason is that we're interested in RV trades with similar notional and similar spreads. The summary statistics show similar distributions across the security spreads, implying that the CDXIG5 nominal values below the minima of the other two securities are practically useless.\n",
    "\n",
    "**Duplicate rows for each security:**  \n",
    "Although there is latency from physical restrictions within exchanges, the time accuracy within the data is 1 second, which is much larger than the average latency in the real world. Therefore, duplicate records for a security at exactly the same timestamp are possibly different trades rather than data errors. For instance, it is possible that one of the duplicates for ITXEB5 is part of an RV trade with CDXIG5 and the other a different RV trade with ITXES5. On the other hand, we're not interested in RV trades between CDXIG5 and ITXES5. As a result, the duplicates for these securities can be dropped to improve code efficiency.\n",
    "\n",
    "**Potential scale differences in spreads/notionals:**  \n",
    "According to the instructions, scale differences between the spreads of two securities within an RV trade are inversely proportional to scale differences in the nominals. Therefore, the product of spread and nominal is relatively invariant to scale differences. The log of this product (to mitigate scale effects from the average nominal values) is the variable that identifies relative value trades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9508c2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "df = pd.read_csv(\"credit_derivatives_trades.csv\").drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# convert timestamp to datetime and define date variable\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "df['date'] = df['timestamp'].dt.date\n",
    "\n",
    "# sanity check\n",
    "df = df[df['notional'] > 0]\n",
    "\n",
    "# sort by timestamp\n",
    "df = df.sort_values(by='timestamp')\n",
    "\n",
    "# drop the duplicates of CDXIG5 and ITXES5, keep the duplicates of ITXEB5\n",
    "df = df[~((df['timestamp'].duplicated()) &\n",
    "          (df['ticker'].isin(['CDXIG5', 'ITXES5'])))]\n",
    "\n",
    "# create auxiliary variable as the product of spread and nominal,\n",
    "# take the log to remove scale effects\n",
    "df['log_sn'] = np.log(df['spread'] * df['notional'])\n",
    "\n",
    "# summary statistics of processed data\n",
    "data = df.copy()\n",
    "prc = [0.01, 0.25, 0.5, 0.75, 0.99]\n",
    "print('\\033[1m' + 'Summary Statistics' + '\\033[0m')\n",
    "display(data.groupby('ticker').describe(percentiles=prc).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd92664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrices of daily median values on\n",
    "# spread and nominal across the three securities\n",
    "dft = data.groupby(['ticker', 'date']).median().reset_index()\n",
    "corr_s = dft.pivot(index='date', columns='ticker', values='spread').corr()\n",
    "corr_n = dft.pivot(index='date', columns='ticker', values='notional').corr()\n",
    "\n",
    "# plot the correlation heatmaps\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(5, 6))\n",
    "heatmap_s = sns.heatmap(round(corr_s, 2), vmin=-1, vmax=1, annot=True, ax=ax1, cmap='viridis')\n",
    "heatmap_s.set_title('Correlations Among Daily Median Spreads', fontdict={'fontsize': 15}, pad=12)\n",
    "heatmap_n = sns.heatmap(round(corr_n, 2), vmin=-1, vmax=1, annot=True, ax=ax2, cmap='viridis')\n",
    "heatmap_n.set_title('Correlations Among Daily Median Nominals', fontdict={'fontsize': 15}, pad=12)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cea162",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### Daily RV trade indicator\n",
    "\n",
    "The granularity limitations in the time stamp works in our favor. The two trades of the RV strategy that happen within a time lag less than a second will be recorded in the data with identical timestamps. Identifying the trade pairs with identical timestamps (either with CDXIG5/ITXEB5 or ITXES5/ITXEB5) gives the minimum number of confirmed RV trades.\n",
    "\n",
    "Certain trade pairs of valid RV trades may have time lags greater than a second. The key to identifying them is to search near the timestamps of previously confirmed RV trades (those with identical timestamps). The intuition is that arbitrageurs attempt to exploit the profitable opportunities as soon as they arise in the market. Therefore, it is unlikely to have in the data isolated RV trades with time lags larger than 1 second and no other arbitrageur has identified the profitable opportunity around the same time.\n",
    "\n",
    "The definition of \"around the same time\" is arbitrary. The time window considered for candidate trade pairs with >1sec lags is 15 minutes before and after the timestamps of the initially confirmed RV trades.\n",
    "\n",
    "The product of spread x nominal (SxN) should be similar for a valid trade pair. Define $Δlog(SxN) <= 0.5$ as the similarity threshold. Visual inspection of the data gives similar nominal values between the two securities when this limit is enforced for the differences in *log_sn*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a2d9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: get the minimum number of confirmed RV trades from\n",
    "# trade pairs with identical timestamps in the raw data\n",
    "conf, tstamps = rv_trade_count(data)\n",
    "conf = conf.rename(columns={'RV_trades': 'min_RV_trades'})\n",
    "\n",
    "# step 2: add the RV trades with timestamps within 15 minutes BEFORE the\n",
    "# confirmed trades of step 1 (use bfill to define the group timestamp)\n",
    "dft = tstamps.copy()\n",
    "dft['group'] = dft['timestamp'] - pd.Timedelta(minutes=15)\n",
    "dft = pd.merge(data, dft.drop(columns='date'), on='timestamp', how='left').bfill().dropna()\n",
    "dft = dft[dft['timestamp'] >= dft['group']]\n",
    "dft = dft.drop(columns='timestamp').rename(columns={'group': 'timestamp'})\n",
    "\n",
    "dfL, _ = rv_trade_count(dft)\n",
    "dfL = dfL.rename(columns={'RV_trades': 'RV_trades_L'})\n",
    "dfL = pd.merge(conf, dfL, on='date', how='outer')\n",
    "\n",
    "# step 3: add the RV trades with timestamps within 15 minutes AFTER the\n",
    "# confirmed trades of step 1 (use ffill to define the group timestamp)\n",
    "dft = tstamps.copy()\n",
    "dft['group'] = dft['timestamp'] + pd.Timedelta(minutes=15)\n",
    "dft = pd.merge(data, dft.drop(columns='date'), on='timestamp', how='left').ffill().dropna()\n",
    "dft = dft[dft['timestamp'] <= dft['group']]\n",
    "dft = dft.drop(columns='timestamp').rename(columns={'group': 'timestamp'})\n",
    "\n",
    "dfU, _ = rv_trade_count(dft)\n",
    "dfU = dfU.rename(columns={'RV_trades': 'RV_trades_U'})\n",
    "\n",
    "# step 4: add the RV trades to get the total daily count\n",
    "tot = pd.merge(dfL, dfU, on='date', how='outer')\n",
    "tot['RV_trades'] = tot.sum(axis=1).astype(int)\n",
    "tot = tot[['date', 'min_RV_trades', 'RV_trades']]\n",
    "\n",
    "# print results\n",
    "print('\\033[1m' + 'Daily RV trade indicator' + '\\033[0m')\n",
    "display(tot)\n",
    "print('\\033[1m' + 'min_RV_trades:' + '\\033[0m' +\n",
    "      ' daily RV trades where the trade pairs have identical timestamps')\n",
    "print('\\033[1m' + 'RV_trades:' + '\\033[0m' + ' total daily RV trades')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
