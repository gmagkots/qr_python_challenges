{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9579eff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from scipy.spatial.distance import cdist\n",
    "from statsmodels.api import add_constant, OLS\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2008d090",
   "metadata": {},
   "source": [
    "___\n",
    "### Auxiliary functions for data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217cc5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_outliers(df, cols, cutoff=3):\n",
    "    \"\"\"\n",
    "    Removes outliers detected across multiple dimensions based on their\n",
    "    Mahalanobis distance from the sample's multivariate mean.\n",
    "\n",
    "    The input dataframe must have a monotonic index starting from zero.\n",
    "\n",
    "    :param df: dataframe with input data\n",
    "    :param cutoff: outlier cutoff threshold\n",
    "    :return: dataframe multivariate outlier rows removed\n",
    "    \"\"\"\n",
    "    # features' mean vector and inverse covariance matrix\n",
    "    mean = df[cols].mean().values.reshape(1, -1)\n",
    "    vi = np.linalg.inv(df[cols].cov())\n",
    "\n",
    "    # estimate the Mahalanobis distance for every sample point\n",
    "    md = cdist(mean, df[cols], 'mahalanobis', VI=vi)\n",
    "\n",
    "    # filter each row by MD and remove it if it's past the threshold value\n",
    "    df = pd.concat([df, pd.DataFrame(md.T, columns=['md'])], axis=1)\n",
    "    df = df[df['md'] <= cutoff].drop(columns=['md'])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def corr_heatmap(df1, df2):\n",
    "    \"\"\"\n",
    "    Plots the feature correlation heatmaps with/without outliers.\n",
    "\n",
    "    :param df1: dataframe with input data without outliers\n",
    "    :param df2: dataframe with input data with outliers\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # create heatmap without outliers\n",
    "    sns.set(font_scale=1.1)\n",
    "    h1 = sns.heatmap(round(df1.corr(), 2), vmin=-1, ax=ax1,\n",
    "                     vmax=1, annot=True, cmap='viridis')\n",
    "    h1.set_title('Feature Correlations',\n",
    "                 fontdict={'fontsize': 15}, pad=12)\n",
    "    h1.set_xticklabels(h1.get_xticklabels(), rotation=45)\n",
    "\n",
    "    # create heatmap with outliers\n",
    "    h2 = sns.heatmap(round(df2.corr(), 2), vmin=-1, ax=ax2,\n",
    "                     vmax=1, annot=True, cmap='viridis')\n",
    "    h2.set_title('Feature Correlations (With Outliers)',\n",
    "                 fontdict={'fontsize': 15}, pad=12)\n",
    "    h2.set_xticklabels(h2.get_xticklabels(), rotation=45)\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def scatter_plots(df1, df2):\n",
    "    \"\"\"\n",
    "    Scatter plots of key features and price with/without outliers.\n",
    "\n",
    "    :param df1: dataframe with input data without outliers\n",
    "    :param df2: dataframe with input data with outliers\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # create plot without outliers\n",
    "    h1 = ax1.scatter(x=df1['log(mileage)'], y=df1['year'],\n",
    "                     c=df1['log(price)'], cmap='turbo')\n",
    "    ax1.set_xlabel('Log10(Mileage)', fontsize=22)\n",
    "    ax1.set_ylabel('Year', fontsize=22)\n",
    "    ax1.set_title('Car Prices', fontdict={'fontsize': 25}, pad=12)\n",
    "    ax1.set_yticks(np.arange(1940, 2030, 10), fontsize=20)\n",
    "    ax1.set_xticks(np.arange(0, 7), fontsize=20)\n",
    "    fig.colorbar(h1, ax=ax1, orientation='vertical', label='Log10(Price)')\n",
    "\n",
    "    # create plot that includes outliers\n",
    "    h2 = ax2.scatter(x=df2['log(mileage)'], y=df2['year'],\n",
    "                     c=df2['log(price)'], cmap='turbo')\n",
    "    ax2.set_xlabel('Log10(Mileage)', fontsize=22)\n",
    "    ax2.set_ylabel('Year', fontsize=22)\n",
    "    ax2.set_title('Car Prices (With Outliers)',\n",
    "                  fontdict={'fontsize': 25}, pad=12)\n",
    "    ax2.set_yticks(np.arange(1940, 2030, 10), fontsize=20)\n",
    "    ax2.set_xticks(np.arange(0, 7), fontsize=20)\n",
    "    fig.colorbar(h2, ax=ax2, orientation='vertical', label='Log10(Price)')\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "\n",
    "    # create scatter plot with car types\n",
    "    markers = ['o', '^', 's', 'D', 'x', 'p']\n",
    "    sns.lmplot(x='log(mileage)', y='year', data=df1, hue='aggcartype',\n",
    "               fit_reg=False, legend=False, markers=markers, palette=\"Set1\",\n",
    "               height=9, aspect=1.1)\n",
    "    plt.xlabel('Log10(Mileage)', fontsize=22)\n",
    "    plt.ylabel('Year', fontsize=22)\n",
    "    plt.title('Car Types', fontdict={'fontsize': 25}, pad=12)\n",
    "    plt.legend(title='Car Type', fontsize=18,\n",
    "               loc='lower left', markerscale=2)\n",
    "    plt.rcParams['legend.title_fontsize'] = 22\n",
    "    plt.yticks(np.arange(1980, 2025, 5), fontsize=20)\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.xlim(1, 5.5)\n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd4584d",
   "metadata": {},
   "source": [
    "___\n",
    "### Data parsing and processing\n",
    "\n",
    "The dataset includes only a few cars with a time series of updates in the platform. This indicates that it is a cross-sectional dataset and the limited time series components from a minority of cars are removed. Only the most recent record is maintained for these cars. The car types are aggregated to broader categories that have a more uniform distribution compared to the raw data. The aggregation also provides for parsimonious models with fewer dummy variables for this feature.\n",
    "\n",
    "The main challenge with identifying good opportunities is to disambiguate what data points consitute outliers and which ones don't. For instance, a 20-year-old car could have its engine changed after service, implying that it would have a very low mileage. In addition, an antique car could be very expensive as a luxury item. I apply a first round of sanity checks on prices, mileage and engine sizes to trim extreme outliers with unrealistic values (~5000 cars). However, this barely helps in addressing the main challenge. Even more conservative statistical criteria with univariate measures such as the Median Absolute Deviation are inadequate in removing all the outliers.\n",
    "\n",
    "Car buyers usually consider a multitude of features when looking for a good deal. For instance, a price by itself is uninformative about the quality of the deal unless it is assessed in parallel with the car's year, mileage, etc. Univariate outlier measures fail for the same reason for this dataset, because the data is fraught with multivariate outliers (see also the comparative scatter plots with and without the outliers). I apply a multivariate measure based on the Mahalanobis Distance (MD) of each data point from the multi-dimensional sample mean. The features I consider include price, mileage and year.\n",
    "\n",
    "The MD metric is the multivariate extension to the univariate Z-score metric for outlier treatment. As such, it is subject to the same robustness concerns as the Z-score. MD relies on sample means and the covariance matrix of the features. Both of these can be affected substantially by outliers, especially in non-normal data that exhibit nonlinear relationships (notice the L-shaped pattern in scatter plots). However, there is no simple multivariate extension of univariate robust measures such as MAD.\n",
    "\n",
    "The MD criteria across price, mileage and year remove roughly 1500 cars, a relatively small loss compared to the remaining 15000 on the platform. However, the improvement in the quality of the sample is substantial. The cross-correlations among these features increase and many of the seemingly good deals (near the top-left quadrant in the scatter plot, low mileage for small car age) are dropped as spurious. A large part of the L-shaped nonlinearity in the scatter plots is also mitigated, although not eradicated completely.\n",
    "\n",
    "The raw data have a very particular structure and the outlier criteria have mitigated some of its irregularities. The criteria cannot be too conservative, otherwise most points would be treated as outliers. It is best practice to have less stringent criteria and use modeling tools that have a degree of robustness to outliers. For instance, random forests might be more suitable for this amount of features and data structure compare to boosted trees. Random forests tend to be robust to outliers because the algorithm ensembles across multiple weak learners. The rationale is that the outliers will give a few bad estimators whose effect will be dimished during ensembling. Contrast to boosted trees that put larger weights on the residual of the weakest estimators in the previous round. The weakest estimators are expected to be those that are affected by outliers, implying that the algorithm assigns larger weight to the outliers compared to the rest of the sample.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01ff770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset and define year feature\n",
    "df = pd.read_csv(\"listing_cars.csv\").drop(columns=['Unnamed: 0'])\n",
    "df['year'] = df['name'].str.lstrip().str[:5].astype(int)\n",
    "\n",
    "# convert date strings to datetime\n",
    "df['lastupdated'] = pd.to_datetime(df['lastupdated'])\n",
    "\n",
    "# sanity checks on numeric values, truncate outliers\n",
    "# with intuitive hardwired thresholds\n",
    "df = df[df['usd_price'] >= 1e3]\n",
    "df = df[df['enginesize'].between(800, 10000)]\n",
    "df = df[df['km_mileage'] < 1e6]\n",
    "\n",
    "# mitigate scale effects by log-transforms\n",
    "scols = ['usd_price', 'km_mileage', 'enginesize']\n",
    "df[['log(price)', 'log(mileage)', 'log(enginesize)']] = np.log10(df[scols])\n",
    "\n",
    "# aggregate car types to broader categories\n",
    "suv = ['SUV/Crossover', 'AWD/4WD']\n",
    "luxury = ['Luxury', 'Convertible', 'Hybrid/Electric']\n",
    "cycle_truck = ['Van/Minivan', 'Wagon/Touring/Estate', 'Motorcycle', 'Truck']\n",
    "df['aggcartype'] = np.where(\n",
    "    df['cartype'].isin(suv), 'SUV', np.where(\n",
    "        df['cartype'].isin(luxury), 'Luxury', np.where(\n",
    "            df['cartype'].isin(cycle_truck), 'Cycle/Truck', df['cartype'])))\n",
    "\n",
    "# sort the data by car and date\n",
    "cols = [col for col in df.columns.tolist()\n",
    "        if col not in ['lastupdated', 'status']]\n",
    "df = df.sort_values(by=cols + ['lastupdated'])\n",
    "\n",
    "# # inspect duplicates, i.e. group by car with >1 rows\n",
    "# dft = df[df.duplicated(subset=cols, keep=False)]\n",
    "\n",
    "# keep only the most recently updated row for each car\n",
    "df = df.drop_duplicates(subset=cols, keep='last')\n",
    "\n",
    "# clean up some unnecessary columns\n",
    "rcols = ['cartype', 'exactmodel', 'seats', 'bodyinteriorcolour',\n",
    "         'registrationexpiry', 'lastserviced', 'model',\n",
    "         'usd_price', 'km_mileage', 'enginesize']\n",
    "df = df.drop(columns=rcols).reset_index(drop=True)\n",
    "\n",
    "# remove multivariate outliers\n",
    "cols = ['log(price)', 'year', 'log(mileage)', 'log(enginesize)']\n",
    "df_outliers = df.copy()\n",
    "df = multivariate_outliers(df, cols=cols, cutoff=3)\n",
    "\n",
    "# summary statistics (numeric and some categorical features)\n",
    "prc = [0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99]\n",
    "s = 'Summary Statistics from a Subset of {} Features'\n",
    "print('\\033[1m' + s.format('Numeric') + '\\033[0m')\n",
    "summ = df.describe(percentiles=prc)\n",
    "summ.loc['missing'] = df[summ.columns].isna().sum()\n",
    "summ.loc['NaN/tot (%)'] = round(summ.loc['missing'] /\n",
    "                                summ.loc['count'] * 100, 2)\n",
    "display(summ)\n",
    "\n",
    "print('\\033[1m' + s.format('Categorical') + '\\033[0m')\n",
    "idxs = ['Automatic', 'Manual', 'Coupe', 'Cycle/Truck',\n",
    "        'Hatchback', 'Luxury', 'SUV', 'Sedan']\n",
    "display(df[['aggcartype', 'transmission']]\n",
    "        .apply(pd.value_counts).reindex(idxs)\n",
    "        .replace(np.nan, 0).astype(int)\n",
    "        .replace(0, '-----', regex=True))\n",
    "\n",
    "# plot the correlation heatmaps and scatter plots\n",
    "corr_heatmap(df, df_outliers)\n",
    "scatter_plots(df, df_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edbd91b",
   "metadata": {},
   "source": [
    "___\n",
    "### Auxiliary functions for identifying good opportunities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2f06be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_model(df_train, df_test):\n",
    "    \"\"\"\n",
    "    Performs a random forest regression of the log-price on various features.\n",
    "\n",
    "    :param df_train: training dataframe\n",
    "    :param df_test: testing dataframe\n",
    "    :return: testing dataframe with log-price predictions from the model\n",
    "    \"\"\"\n",
    "    # setup the feature names\n",
    "    dummies = [col for col in df.columns if\n",
    "               col.startswith('type_') or col.startswith('gear_')]\n",
    "    xcols = ['log(mileage)', 'year', 'previousowners',\n",
    "             'log(enginesize)', 'ord_brand'] + dummies\n",
    "    xycols = xcols + ['log(price)']\n",
    "\n",
    "    # remove missing values\n",
    "    df_train = df_train.dropna(subset=xycols)\n",
    "    df_test = df_test.dropna(subset=xycols)\n",
    "\n",
    "    # load the random forest regressor class\n",
    "    rf = RandomForestRegressor(n_estimators=15, max_depth=3, random_state=0)\n",
    "\n",
    "    # fit the model in the training set\n",
    "    rf.fit(X=df_train[xcols], y=df_train['log(price)'])\n",
    "\n",
    "    # get the model's log-price predictions in the testing set\n",
    "    yhat = rf.predict(df_test[xcols])\n",
    "\n",
    "    # estimate OOS R-squared by OLS regression between observed\n",
    "    # and predicted values in the testing set, the intercept\n",
    "    # enforces the metric within [0, 1]\n",
    "    R2 = OLS(df_test['log(price)'], add_constant(yhat),\n",
    "             missing='drop').fit().rsquared\n",
    "    print('\\n\\033[1m' + 'Random Forest OOS Model Performance R2 = {}'\n",
    "          .format(R2.round(2)) + '\\033[0m')\n",
    "\n",
    "    # place the predicted duration in the testing dataframe\n",
    "    df_test['yhat'] = yhat\n",
    "\n",
    "    # visualize feature importance results with Shapley values\n",
    "    explainer = shap.TreeExplainer(rf)\n",
    "    shap_values = explainer.shap_values(df_test[xcols])\n",
    "    shap.summary_plot(shap_values, df_test[xcols], plot_type=\"bar\")\n",
    "    plt.show()\n",
    "\n",
    "    return df_test\n",
    "\n",
    "\n",
    "def ranking_output(df):\n",
    "    \"\"\"\n",
    "    Summarizes the good/bad car opportunities and their ranking.\n",
    "\n",
    "    :param df: dataframe with model predictions\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # clean up\n",
    "    cols = [col for col in df.columns if\n",
    "            col.startswith('type_') or col.startswith('gear_')] + ['ord_brand']\n",
    "    df = df.drop(columns=cols)\n",
    "\n",
    "    # separate the good from the bad opportunities\n",
    "    df['y-yhat'] = df['log(price)'] - df['yhat']\n",
    "    df_good = df[df['y-yhat'] <= 0].sort_values(by='y-yhat', ascending=False)\n",
    "    df_bad = df[df['y-yhat'] > 0].sort_values(by='y-yhat', ascending=False)\n",
    "\n",
    "    # create rank column\n",
    "    df_good['ranking'] = df_good['y-yhat'].rank(method='first').astype(int)\n",
    "    df_bad['ranking'] = df_bad['y-yhat'].rank(method='first').astype(int)\n",
    "\n",
    "    # change the order of columns in the output frames\n",
    "    cols = ['ranking', 'y-yhat'] + [col for col in df.columns\n",
    "                                    if col not in ['ranking', 'y-yhat']]\n",
    "\n",
    "    # sort the output dataframes by rank\n",
    "    df_good = df_good[cols].sort_values(by='ranking')\n",
    "    df_bad = df_bad[cols].sort_values(by='ranking')\n",
    "\n",
    "    # show the good/bad opportunities in descending order\n",
    "    print('\\n\\033[1m' + 'Good Opportunities (Underpriced Cars): ' +\n",
    "          'ranked best to least good' + '\\033[0m')\n",
    "    display(df_good)\n",
    "    print('\\n\\033[1m' + 'Bad Opportunities (Overpriced Cars): ' +\n",
    "          'ranked least bad to worst' + '\\033[0m')\n",
    "    display(df_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bae78dc",
   "metadata": {},
   "source": [
    "___\n",
    "### Finding good opportunities\n",
    "\n",
    "The train/test data split is straighforward for this data. Sold cars are used for training the model and its predictions are tested against the Active cars on the platform. According to the model, a good opportunity arises when the marketed price is smaller than the model's prediction for this car, while a bad opportunity will have a marketed price that is higher than the predicted one. In particular, if $y$ and $\\hat y$ are the marketed and predicted prices respectively, then\n",
    "1. $y$ - $\\hat y$ < 0 => good opportunity (car is underpriced)\n",
    "2. $y$ - $\\hat y$ > 0 => bad opportunity (car is overpriced)\n",
    "\n",
    "The list of features includes log-mileage, year, an aggregated car type, brand, transmission details, number of previous owners and log-engine size. Car type and transmission have small numbers of possible values and they are encoded as dummy variables. On the contrary, brand takes too many possible values for one-hot encoding, and it is transformed into frequency-based ordinal IDs. The random forest model's feature analysis demonstrates that the only relevant features for predicting car prices are mileage, year and engine size. The relationship perhaps wouldn't have been as clear if the multivariate outliers across these three features and log-price hadn't been removed.\n",
    "\n",
    "The output is split into two subsets. The first included the good opportunities ordered from best to worst. The second includes the overpriced cars ordered from least bad to worst possible opportunity. The ranking is directly related to the value of the difference $y$ - $\\hat y$. For the good deals, the more negative this difference is, the more underpriced the car, and vice versa for the bad deals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03227aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create car type and transmission dummies\n",
    "df[['type', 'gear']] = df[['aggcartype', 'transmission']]\n",
    "df = pd.get_dummies(df, columns=['type', 'gear'], drop_first=True, dtype=int)\n",
    "\n",
    "# define ordinal brand IDs from their frequency\n",
    "df['ord_brand'] = df.groupby('brand')['brand'].transform('count')\n",
    "\n",
    "# split main data into training and testing subsets\n",
    "# (save a copy of the training test for household finance question)\n",
    "df_train, df_test = df[df['status'] == 'Sold'], df[df['status'] == 'Active']\n",
    "df_fin = df_train.copy()\n",
    "\n",
    "# fill in the model-predicted prices in the testing dataset\n",
    "dfp = random_forest_model(df_train, df_test)\n",
    "\n",
    "# show results\n",
    "ranking_output(dfp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d5f773",
   "metadata": {},
   "source": [
    "___\n",
    "### Weekly signal on household finances\n",
    "\n",
    "A likely indicator variable for the state of household finances (and perhaps the consumer economy by extension) is the number of weekly car sales. Its histogram reveals broad variation across weeks, along with some clustering as well. This analysis focuses on the training dataset that contains confirmed sales, unlike marketed prices in the training set.\n",
    "\n",
    "Candidate signals in the training dataset include the cross-sectional averages and standard deviations of sale prices and the features that were shown predictability characteristics in the model before (log-mileage, log-engine size and year of the car make. Following a grid search for the best performing combination of these features, the proposed feature is the ratio $\\sigma_{km}/\\sigma_{eng}$, where $\\sigma_{km}$ and $\\sigma_{eng}$ are the standard deviations of the log-mileage and log-engine size respectively. The coefficient of total weekly sales on weekly-lagged values of the signal is positive.\n",
    "\n",
    "The signal implies that an increase in car mileage variance and a decrease of engine size variance predict a surge in car sales in the following week, and by extension imply an improved state of household finances. The intuition of the signal's predictability is the following:\n",
    "1. An increase in the dispersion of mileage $\\sigma_{km}$ among sold cars implies that the buyers' interest is spread across used and new cars. The new cars tend to be more expensive, implying that households can afford to buy such cars.\n",
    "2. A simultaneous decrease in engine size dispersion $\\sigma_{eng}$ implies an equilibrium of supply with a demand side (the buyers) that targets specific car specification. In other words, the buyers know what they want and are willing to purchase it, rather than exploring alternative car specifications within a lower budget.\n",
    "3. The combination of these two economic forces among the buyers (demand for new and expensive cars with targeted specifications) could indicate that households are strong enough to result in a surge in sold cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9f5a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate dates into weekly intervals throughout the year\n",
    "df_fin['Week'] = df_fin['lastupdated'].dt.isocalendar().week\n",
    "\n",
    "# weekly car sales proxy the state of household finances\n",
    "wk_sales = df_fin.groupby(df_fin['Week'])['name'].count()\n",
    "wk_sales.plot(kind='bar', legend=False, figsize=(10, 8),\n",
    "              xticks=range(0, 53, 2), ylabel='Total Weekly Sales',\n",
    "              title='Weekly Sales Distribution', rot=0, fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037b00e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid of candidate signals\n",
    "wk_avg_sale_price = df_fin.groupby(df_fin['Week'])['log(price)'].mean()\n",
    "wk_avg_mileage = df_fin.groupby(df_fin['Week'])['log(mileage)'].mean()\n",
    "wk_avg_engine = df_fin.groupby(df_fin['Week'])['log(enginesize)'].mean()\n",
    "wk_avg_year = df_fin.groupby(df_fin['Week'])['year'].mean()\n",
    "wk_std_sale_price = df_fin.groupby(df_fin['Week'])['log(price)'].std()\n",
    "wk_std_mileage = df_fin.groupby(df_fin['Week'])['log(mileage)'].std()\n",
    "wk_std_engine = df_fin.groupby(df_fin['Week'])['log(enginesize)'].std()\n",
    "wk_std_year = df_fin.groupby(df_fin['Week'])['year'].std()\n",
    "\n",
    "# best performing signal\n",
    "signal = (wk_std_mileage / wk_std_engine).rename('sigma(mileage)/sigma(engine)')\n",
    "res = OLS(wk_sales, add_constant(signal.shift()), missing='drop').fit()\n",
    "print('\\n\\033[1m' + 'OOS performance for best-performing signal R2 = {}'\n",
    "      .format(res.rsquared.round(4)) + '\\033[0m')\n",
    "print('\\n\\033[1m' + 'Model parameters' + '\\033[0m')\n",
    "res.params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
